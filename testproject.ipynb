{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.16.4)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: hyperopt in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.2.5)\n",
      "Requirement already satisfied: pytz>=2011k in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: cloudpickle in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from hyperopt) (1.6.0)\n",
      "Requirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from hyperopt) (4.56.2)\n",
      "Requirement already satisfied: networkx>=2.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from hyperopt) (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from networkx>=2.2->hyperopt) (4.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas scikit-learn numpy nltk hyperopt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>amazon-id</th>\n",
       "      <th>helpful</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>root-genre</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>label</th>\n",
       "      <th>first-release-year</th>\n",
       "      <th>songs</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4984057859803657856</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1302739200</td>\n",
       "      <td>Very nice music for practicing my Tai Chi. I d...</td>\n",
       "      <td>4</td>\n",
       "      <td>04 14, 2011</td>\n",
       "      <td>Beautiful</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>New Age</td>\n",
       "      <td>-3267874170410107454</td>\n",
       "      <td>-7180760356347753735</td>\n",
       "      <td>Cdbaby/Cdbaby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9136764282801708742</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[11, 11]</td>\n",
       "      <td>1180396800</td>\n",
       "      <td>I recently starting doing Tai Chi which I love...</td>\n",
       "      <td>5</td>\n",
       "      <td>05 29, 2007</td>\n",
       "      <td>Tranquillity In Motion !!!</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>New Age</td>\n",
       "      <td>-3267874170410107454</td>\n",
       "      <td>-7180760356347753735</td>\n",
       "      <td>Cdbaby/Cdbaby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2164551966908582519</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1361404800</td>\n",
       "      <td>My wife uses it for her class room the kids lo...</td>\n",
       "      <td>5</td>\n",
       "      <td>02 21, 2013</td>\n",
       "      <td>Great Stuff</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>New Age</td>\n",
       "      <td>-3267874170410107454</td>\n",
       "      <td>-7180760356347753735</td>\n",
       "      <td>Cdbaby/Cdbaby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7309200698931694843</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>1338163200</td>\n",
       "      <td>We bought this music to go Dr Lam DVD. The mus...</td>\n",
       "      <td>5</td>\n",
       "      <td>05 28, 2012</td>\n",
       "      <td>Beautiful</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>New Age</td>\n",
       "      <td>-3267874170410107454</td>\n",
       "      <td>-7180760356347753735</td>\n",
       "      <td>Cdbaby/Cdbaby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4461682407031037732</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1396310400</td>\n",
       "      <td>It helps me do my exercise because it sets the...</td>\n",
       "      <td>5</td>\n",
       "      <td>04 1, 2014</td>\n",
       "      <td>tai chi music</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>New Age</td>\n",
       "      <td>-3267874170410107454</td>\n",
       "      <td>-7180760356347753735</td>\n",
       "      <td>Cdbaby/Cdbaby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID            amazon-id   helpful  unixReviewTime  \\\n",
       "0 -4984057859803657856  1877521326299865484    [2, 2]      1302739200   \n",
       "1  9136764282801708742  1877521326299865484  [11, 11]      1180396800   \n",
       "2  2164551966908582519  1877521326299865484    [0, 0]      1361404800   \n",
       "3 -7309200698931694843  1877521326299865484    [4, 4]      1338163200   \n",
       "4 -4461682407031037732  1877521326299865484    [0, 0]      1396310400   \n",
       "\n",
       "                                          reviewText  overall   reviewTime  \\\n",
       "0  Very nice music for practicing my Tai Chi. I d...        4  04 14, 2011   \n",
       "1  I recently starting doing Tai Chi which I love...        5  05 29, 2007   \n",
       "2  My wife uses it for her class room the kids lo...        5  02 21, 2013   \n",
       "3  We bought this music to go Dr Lam DVD. The mus...        5  05 28, 2012   \n",
       "4  It helps me do my exercise because it sets the...        5   04 1, 2014   \n",
       "\n",
       "                      summary  price                  categories root-genre  \\\n",
       "0                   Beautiful  16.47  ['CDs & Vinyl', 'New Age']    New Age   \n",
       "1  Tranquillity In Motion !!!  16.47  ['CDs & Vinyl', 'New Age']    New Age   \n",
       "2                 Great Stuff  16.47  ['CDs & Vinyl', 'New Age']    New Age   \n",
       "3                   Beautiful  16.47  ['CDs & Vinyl', 'New Age']    New Age   \n",
       "4               tai chi music  16.47  ['CDs & Vinyl', 'New Age']    New Age   \n",
       "\n",
       "                 title               artist          label  \\\n",
       "0 -3267874170410107454 -7180760356347753735  Cdbaby/Cdbaby   \n",
       "1 -3267874170410107454 -7180760356347753735  Cdbaby/Cdbaby   \n",
       "2 -3267874170410107454 -7180760356347753735  Cdbaby/Cdbaby   \n",
       "3 -3267874170410107454 -7180760356347753735  Cdbaby/Cdbaby   \n",
       "4 -3267874170410107454 -7180760356347753735  Cdbaby/Cdbaby   \n",
       "\n",
       "   first-release-year                                              songs  \\\n",
       "0                 NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "1                 NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "2                 NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "3                 NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "4                 NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "\n",
       "   salesRank                                            related  \n",
       "0      27222  {'also_bought': [-404470919165672227, 11968160...  \n",
       "1      27222  {'also_bought': [-404470919165672227, 11968160...  \n",
       "2      27222  {'also_bought': [-404470919165672227, 11968160...  \n",
       "3      27222  {'also_bought': [-404470919165672227, 11968160...  \n",
       "4      27222  {'also_bought': [-404470919165672227, 11968160...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "handle_text: takes out stop words and punctuation, lemmatizes, and converts to lowercase\n",
    "* input: string\n",
    "* output: hanlded string\n",
    "\"\"\"\n",
    "def handle_text(text):\n",
    "    # Check that argument provided is a string\n",
    "    if not isinstance(text,str):\n",
    "        return \"\"\n",
    "    output = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # use stemmer instead of lemmatizer here\n",
    "    # wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sno = nltk.stem.SnowballStemmer('english')\n",
    "    for word in nltk.word_tokenize(text): \n",
    "        # Disclude stop words and words with punctuation\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            # Add lemmatized words\n",
    "            #output.append(wordnet_lemmatizer.lemmatize(word))\n",
    "            output.append(sno.stem(word))\n",
    "    return \" \".join(output) # return the list of words as one single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convert_text: applies handle_text to all summaries and reviews in dataframe\n",
    "* input: dataframe\n",
    "* output: new dataframe with converted summaries/reviews\n",
    "\"\"\"\n",
    "def convert_text(df):\n",
    "    df = df.assign(summary=df['summary'].apply(handle_text))\n",
    "    df = df.assign(reviewText=df['reviewText'].apply(handle_text))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prep_df: applies convert text to both summary and review features\n",
    "* input: dataframe\n",
    "* output: new dataframe with converted summaries/reviews\n",
    "\"\"\"\n",
    "\n",
    "def prep_df(df):\n",
    "    # Convert to lowercase, remove punctuation and stop words, lemmatize words\n",
    "    # df = convert_text(df)\n",
    "    # uncommon_words = get_uncommon_words(df['reviewText'], 2)\n",
    "    # Remove uncommon words\n",
    "    #return remove_words_reviews(df, uncommon_words)\n",
    "    return convert_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "awesome_products: extracts the training label based on overall ratings of the products in the dataset\n",
    "* input: dataframe\n",
    "* output: the training label - y_train\n",
    "\"\"\"\n",
    "\n",
    "def awesome_products(df):\n",
    "    df = df.groupby('amazon-id').agg({'overall': lambda x: 1 if np.mean(x) > 4.5 else 0})\n",
    "    return df['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make_prediction: uses LinearSVC as the base model to predictions based on training data\n",
    "* input: x_train, y_train, x_test\n",
    "* output: the model  used and the predictions from the model\n",
    "\"\"\"\n",
    "\n",
    "def make_prediction(x_train, y_train, x_test):\n",
    "    print(\"Predicting\")\n",
    "    model = LogisticRegression(C=1, penalty='l1', solver='liblinear'))\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Done\")\n",
    "    return model, model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def test_model():\n",
    "    \n",
    "    # gets the training vector and the training label based on the training data\n",
    "    train_vec, awe_vec, ct = transform_train(train)\n",
    "    \n",
    "    # splits the data into the training set and the validation set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_vec, awe_vec)\n",
    "    \n",
    "    #x_train_new , y_train_new = correct_oversampling(x_train, y_train)\n",
    "    \n",
    "    # gets predictions\n",
    "    \n",
    "    model, y_pred = make_prediction(x_train, y_train, x_test)\n",
    "    # print(\"f1_score=\", f1_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\"\"\"\n",
    "transform_train: Allows for feature extraction from multiple columns in the training data using a ColumnTransformer \n",
    "object. The transformer object needs to be told how to transform the columns we're interested in. ct_opt fulfills this\n",
    "purpose. It returns the transformed training data as a vector along with the training label, using the awesome_products \n",
    "method and the column transformer object. This makes sure that we use the same vectorizers to transform summary and review text in the\n",
    "testing data.\n",
    "* input: data\n",
    "* output: the training vector, y label and the column transformer object\n",
    "\"\"\"\n",
    "\n",
    "def transform_train(data): \n",
    "    print(\"Prepping train df\")\n",
    "    data_new = prep_df(data)\n",
    "    print(\"Done\")\n",
    "    combined_df = data_new.groupby('amazon-id').agg({'reviewText': ' '.join, 'summary': ' '.join, 'salesRank': lambda x: np.mean(x), 'price': lambda x: np.mean(x)})\n",
    "    print(\"Feature extraction\")\n",
    "    # change vectorizer!\n",
    "    # improving max features\n",
    "    # using stem instead of lemmatizer \n",
    "    \n",
    "    # Column Transformer options - this list specifies how the columns we're interested in are transformed\n",
    "    # reviewText gets a tfidf vectorizer\n",
    "    # summary gets a tfidf vectorizer too (so that we don't use the same vectorizer for both features)\n",
    "    # salesRank and price are both normalized\n",
    "    \n",
    "    ct_opts = [('reviewText_bow', TfidfVectorizer(max_features = 20000, min_df=2), 'reviewText'),\n",
    "          ('summary_bow', TfidfVectorizer(max_features = 20000, min_df=2), 'summary'),\n",
    "          ('salesRank_norm', MinMaxScaler(), ['salesRank']),\n",
    "          ('price_norm', MinMaxScaler(), ['price'])]\n",
    "    \n",
    "    # The ColumnTransformer object changes the training data in accordance with the options specified above. \n",
    "    # the other features in the dataset are disregarded\n",
    "    ct = ColumnTransformer(ct_opts, remainder = 'drop')\n",
    "    \n",
    "    # fitting the training data based on the column transformer object\n",
    "    res_vec = np.array(ct.fit_transform(combined_df).toarray())\n",
    "    print(\"Done\")\n",
    "    print(\"Prepping y labels\")\n",
    "    awe_vec = awesome_products(data_new)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # Passing ct here so that we can use the same vectorizers on the testing data\n",
    "    return res_vec, awe_vec, ct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "transform_test: Allows for feature extraction from multiple columns in the testing data using a ColumnTransformer \n",
    "object. We use the same transformer object we got from transform train here using the input parameter ct.\n",
    "* input: testing data, the previously used ColumnTransformer \n",
    "* output: the testing data transformed into a vector\n",
    "\"\"\"\n",
    "\n",
    "def transform_test(data, ct):\n",
    "    print(\"Prepping test data\")\n",
    "    data_new = prep_df(data)\n",
    "    print(\"Done\")\n",
    "    print(\"Feature extraction\")\n",
    "    combined_df = data_new.groupby('amazon-id').agg({'reviewText': ' '.join, 'summary': ' '.join, 'salesRank': lambda x: np.mean(x), 'price': lambda x: np.mean(x)})\n",
    "    #ct_opts = [('reviewText_bow', TfidfVectorizer(max_features = 8000, min_df=2), 'reviewText'),\n",
    "    #      ('summary_bow', TfidfVectorizer(max_features = 8000, min_df=2), 'summary'),\n",
    "    #     ('salesRank_norm', MinMaxScaler(), ['salesRank']),\n",
    "    #      ('price_norm', MinMaxScaler(), ['price'])]\n",
    "    # ct = ColumnTransformer(ct_opts, remainder = 'drop')\n",
    "    print(\"Done\")\n",
    "    res_vec = np.array(ct.transform(combined_df).toarray())\n",
    "    return res_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    \n",
    "    # gets the training vector and the training label based on the training data\n",
    "    df_train = pd.read_csv('datasets/Train.csv')\n",
    "    x_train, y_train, ct = transform_train(df_train)\n",
    "\n",
    "    # gets the test vector based on the testing data\n",
    "    df_test = pd.read_csv('datasets/Test.csv')\n",
    "    x_test = transform_test(df_test, ct)\n",
    "\n",
    "    \n",
    "    # makes predictions\n",
    "    model, pred_y = make_prediction(train_vec, train_y, test_vec)\n",
    "    \n",
    "    #saves the predictions to a .csv file - check output before submitting code\n",
    "    save_results(df_test, pred_y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the training vector and the training label based on the training data\n",
    "\n",
    "def save_results(df_test, pred_y):\n",
    "    print(\"Writing output\")\n",
    "    output = pd.DataFrame({'amazon-id': df_test[\"amazon-id\"].drop_duplicates().reset_index(drop=True), 'Awesome': pred_y})\n",
    "    output.to_csv('predictions_test.csv', index= False)\n",
    "    output.head(10)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping train df\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping train df\n",
      "Done\n",
      "Feature extraction\n",
      "Done\n",
      "Prepping y labels\n",
      "Done\n",
      "Prepping test data\n",
      "Done\n",
      "Feature extraction\n",
      "Done\n",
      "Predicting\n",
      "Done\n",
      "Writing output\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "run_model()\n",
    "# The code after this point is stuff I've been playing around with\n",
    "# a: sentiment analysis\n",
    "# b: oversampling class 0 using SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  pd.read_csv('predictions_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>amazon-id</th>\n",
       "      <th>Awesome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3827781478900123339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1146746308871643825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7943165581472649995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9108673782042818101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6254356016991899485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-5853727546265815229</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7870026881680799188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-9080174460600991411</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5049399051352648854</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2944373440361513822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            amazon-id  Awesome\n",
       "0           0 -3827781478900123339        1\n",
       "1           1 -1146746308871643825        0\n",
       "2           2  7943165581472649995        1\n",
       "3           3  9108673782042818101        1\n",
       "4           4  6254356016991899485        1\n",
       "5           5 -5853727546265815229        1\n",
       "6           6  7870026881680799188        1\n",
       "7           7 -9080174460600991411        1\n",
       "8           8  5049399051352648854        1\n",
       "9           9  2944373440361513822        1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ct_opts = [('reviewText_bow', TfidfVectorizer(max_features = 4000), 'reviewText'),\n",
    "          ('summary_bow', TfidfVectorizer(max_features = 4000), 'summary'),\n",
    "          ('salesRank_norm', MinMaxScaler(), ['salesRank']),\n",
    "          ('price_norm', MinMaxScaler(), ['price'])]\n",
    "ct = ColumnTransformer(ct_opts, remainder = 'drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = prep_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = train_new.groupby('amazon-id').agg({'reviewText': ' '.join, 'summary': ' '.join, 'salesRank': lambda x: np.mean(x), 'price': lambda x: np.mean(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon-id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-9217723718720870868</th>\n",
       "      <td>year album aretha franklin released one best a...</td>\n",
       "      <td>come back album one ree best love classy album...</td>\n",
       "      <td>1310516</td>\n",
       "      <td>9.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9215746463819797371</th>\n",
       "      <td>one beautiful instrumental cd ever heard beaut...</td>\n",
       "      <td>amazing piano beautiful album mainlypiano high...</td>\n",
       "      <td>309139</td>\n",
       "      <td>48.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9213978596308513604</th>\n",
       "      <td>july celebrate anniversary release epic souths...</td>\n",
       "      <td>ok great</td>\n",
       "      <td>280309</td>\n",
       "      <td>15.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9211290576571923870</th>\n",
       "      <td>first heard recording released lp spell ever s...</td>\n",
       "      <td>key joyous laughter mediterranean sunshine</td>\n",
       "      <td>321654</td>\n",
       "      <td>17.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9208769561690910545</th>\n",
       "      <td>another rare case score film better actual mov...</td>\n",
       "      <td>great soundtrack halloween resurrection evil f...</td>\n",
       "      <td>17515</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9208198150317071838</th>\n",
       "      <td>another excellent collection song josh ritter ...</td>\n",
       "      <td>excellent josh ritter excellent songwriter per...</td>\n",
       "      <td>114996</td>\n",
       "      <td>33.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9207623668807116759</th>\n",
       "      <td>album compilation los cuates de sinaloa greate...</td>\n",
       "      <td>los cuates de sinaloa breaking bad azul</td>\n",
       "      <td>221487</td>\n",
       "      <td>31.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9203846742259231188</th>\n",
       "      <td>like creative beat eclectic break unkle dj sha...</td>\n",
       "      <td>another unkle classic essential unkle cool unk...</td>\n",
       "      <td>553883</td>\n",
       "      <td>48.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9202734100323412002</th>\n",
       "      <td>superb composition wonderful unique composer p...</td>\n",
       "      <td>exclellent azerbaijani jazz glorious</td>\n",
       "      <td>405958</td>\n",
       "      <td>15.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9198322104429679287</th>\n",
       "      <td>cd expected thing found strange album played p...</td>\n",
       "      <td>good</td>\n",
       "      <td>270367</td>\n",
       "      <td>51.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             reviewText  \\\n",
       "amazon-id                                                                 \n",
       "-9217723718720870868  year album aretha franklin released one best a...   \n",
       "-9215746463819797371  one beautiful instrumental cd ever heard beaut...   \n",
       "-9213978596308513604  july celebrate anniversary release epic souths...   \n",
       "-9211290576571923870  first heard recording released lp spell ever s...   \n",
       "-9208769561690910545  another rare case score film better actual mov...   \n",
       "-9208198150317071838  another excellent collection song josh ritter ...   \n",
       "-9207623668807116759  album compilation los cuates de sinaloa greate...   \n",
       "-9203846742259231188  like creative beat eclectic break unkle dj sha...   \n",
       "-9202734100323412002  superb composition wonderful unique composer p...   \n",
       "-9198322104429679287  cd expected thing found strange album played p...   \n",
       "\n",
       "                                                                summary  \\\n",
       "amazon-id                                                                 \n",
       "-9217723718720870868  come back album one ree best love classy album...   \n",
       "-9215746463819797371  amazing piano beautiful album mainlypiano high...   \n",
       "-9213978596308513604                                           ok great   \n",
       "-9211290576571923870         key joyous laughter mediterranean sunshine   \n",
       "-9208769561690910545  great soundtrack halloween resurrection evil f...   \n",
       "-9208198150317071838  excellent josh ritter excellent songwriter per...   \n",
       "-9207623668807116759            los cuates de sinaloa breaking bad azul   \n",
       "-9203846742259231188  another unkle classic essential unkle cool unk...   \n",
       "-9202734100323412002               exclellent azerbaijani jazz glorious   \n",
       "-9198322104429679287                                               good   \n",
       "\n",
       "                      salesRank  price  \n",
       "amazon-id                               \n",
       "-9217723718720870868    1310516   9.90  \n",
       "-9215746463819797371     309139  48.11  \n",
       "-9213978596308513604     280309  15.86  \n",
       "-9211290576571923870     321654  17.37  \n",
       "-9208769561690910545      17515  12.00  \n",
       "-9208198150317071838     114996  33.61  \n",
       "-9207623668807116759     221487  31.27  \n",
       "-9203846742259231188     553883  48.40  \n",
       "-9202734100323412002     405958  15.08  \n",
       "-9198322104429679287     270367  51.44  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new['sent_scores']= train_new['reviewText'].apply(lambda review: sid.polarity_scores(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/isuruabeysekara/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new['compound']  = train_new['sent_scores'].apply(lambda score_dict: score_dict['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>amazon-id</th>\n",
       "      <th>helpful</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>...</th>\n",
       "      <th>first-release-year</th>\n",
       "      <th>songs</th>\n",
       "      <th>salesRank</th>\n",
       "      <th>related</th>\n",
       "      <th>sent_scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>comp_score</th>\n",
       "      <th>sum_sent_scores</th>\n",
       "      <th>sum_compound</th>\n",
       "      <th>sum_comp_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4984057859803657856</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1302739200</td>\n",
       "      <td>nice music practicing tai chi downloaded phone...</td>\n",
       "      <td>4</td>\n",
       "      <td>04 14, 2011</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.124, 'neu': 0.604, 'pos': 0.272, 'co...</td>\n",
       "      <td>0.7089</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9136764282801708742</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[11, 11]</td>\n",
       "      <td>1180396800</td>\n",
       "      <td>recently starting tai chi love immensily addin...</td>\n",
       "      <td>5</td>\n",
       "      <td>05 29, 2007</td>\n",
       "      <td>tranquillity motion</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.025, 'neu': 0.472, 'pos': 0.503, 'co...</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'comp...</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2164551966908582519</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1361404800</td>\n",
       "      <td>wife us class room kid love loved price great ...</td>\n",
       "      <td>5</td>\n",
       "      <td>02 21, 2013</td>\n",
       "      <td>great stuff</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.317, 'pos': 0.683, 'comp...</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.196, 'pos': 0.804, 'comp...</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7309200698931694843</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>1338163200</td>\n",
       "      <td>bought music go dr lam dvd music perfect give ...</td>\n",
       "      <td>5</td>\n",
       "      <td>05 28, 2012</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.597, 'pos': 0.403, 'comp...</td>\n",
       "      <td>0.8957</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4461682407031037732</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1396310400</td>\n",
       "      <td>help exercise set proper mood happy quality pl...</td>\n",
       "      <td>5</td>\n",
       "      <td>04 1, 2014</td>\n",
       "      <td>tai chi music</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.462, 'pos': 0.538, 'comp...</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7459674557577458961</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1287273600</td>\n",
       "      <td>tai chi music paul lam cd exactly expected mus...</td>\n",
       "      <td>4</td>\n",
       "      <td>10 17, 2010</td>\n",
       "      <td>tai chi music</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound...</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8261414655084321703</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[15, 30]</td>\n",
       "      <td>1107043200</td>\n",
       "      <td>highly admire work paul lam tai chi extremely ...</td>\n",
       "      <td>1</td>\n",
       "      <td>01 30, 2005</td>\n",
       "      <td>love dr lam work sample available</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.227, 'neu': 0.56, 'pos': 0.213, 'com...</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>neg</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.543, 'pos': 0.457, 'comp...</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3180071471897091081</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1395705600</td>\n",
       "      <td>husband getting year previous experience pract...</td>\n",
       "      <td>5</td>\n",
       "      <td>03 25, 2014</td>\n",
       "      <td>love dvd</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.757, 'pos': 0.243, 'comp...</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'comp...</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1423705408082526473</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1379808000</td>\n",
       "      <td>tried tai chi yoga music fitting provides righ...</td>\n",
       "      <td>5</td>\n",
       "      <td>09 22, 2013</td>\n",
       "      <td>ideal music</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.227, 'pos': 0.773, 'comp...</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-4401404055957679750</td>\n",
       "      <td>1877521326299865484</td>\n",
       "      <td>[5, 5]</td>\n",
       "      <td>1382659200</td>\n",
       "      <td>teach tai chi although music pleasant prefer u...</td>\n",
       "      <td>3</td>\n",
       "      <td>10 25, 2013</td>\n",
       "      <td>nice practice</td>\n",
       "      <td>16.47</td>\n",
       "      <td>['CDs &amp; Vinyl', 'New Age']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7058439142327364074, 6037075874942075284, 852...</td>\n",
       "      <td>27222</td>\n",
       "      <td>{'also_bought': [-404470919165672227, 11968160...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'comp...</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>pos</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'comp...</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID            amazon-id   helpful  unixReviewTime  \\\n",
       "0 -4984057859803657856  1877521326299865484    [2, 2]      1302739200   \n",
       "1  9136764282801708742  1877521326299865484  [11, 11]      1180396800   \n",
       "2  2164551966908582519  1877521326299865484    [0, 0]      1361404800   \n",
       "3 -7309200698931694843  1877521326299865484    [4, 4]      1338163200   \n",
       "4 -4461682407031037732  1877521326299865484    [0, 0]      1396310400   \n",
       "5 -7459674557577458961  1877521326299865484    [0, 0]      1287273600   \n",
       "6  8261414655084321703  1877521326299865484  [15, 30]      1107043200   \n",
       "7  3180071471897091081  1877521326299865484    [0, 0]      1395705600   \n",
       "8  1423705408082526473  1877521326299865484    [1, 1]      1379808000   \n",
       "9 -4401404055957679750  1877521326299865484    [5, 5]      1382659200   \n",
       "\n",
       "                                          reviewText  overall   reviewTime  \\\n",
       "0  nice music practicing tai chi downloaded phone...        4  04 14, 2011   \n",
       "1  recently starting tai chi love immensily addin...        5  05 29, 2007   \n",
       "2  wife us class room kid love loved price great ...        5  02 21, 2013   \n",
       "3  bought music go dr lam dvd music perfect give ...        5  05 28, 2012   \n",
       "4  help exercise set proper mood happy quality pl...        5   04 1, 2014   \n",
       "5  tai chi music paul lam cd exactly expected mus...        4  10 17, 2010   \n",
       "6  highly admire work paul lam tai chi extremely ...        1  01 30, 2005   \n",
       "7  husband getting year previous experience pract...        5  03 25, 2014   \n",
       "8  tried tai chi yoga music fitting provides righ...        5  09 22, 2013   \n",
       "9  teach tai chi although music pleasant prefer u...        3  10 25, 2013   \n",
       "\n",
       "                             summary  price                  categories  ...  \\\n",
       "0                          beautiful  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "1                tranquillity motion  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "2                        great stuff  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "3                          beautiful  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "4                      tai chi music  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "5                      tai chi music  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "6  love dr lam work sample available  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "7                           love dvd  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "8                        ideal music  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "9                      nice practice  16.47  ['CDs & Vinyl', 'New Age']  ...   \n",
       "\n",
       "  first-release-year                                              songs  \\\n",
       "0                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "1                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "2                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "3                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "4                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "5                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "6                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "7                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "8                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "9                NaN  [7058439142327364074, 6037075874942075284, 852...   \n",
       "\n",
       "   salesRank                                            related  \\\n",
       "0      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "1      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "2      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "3      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "4      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "5      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "6      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "7      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "8      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "9      27222  {'also_bought': [-404470919165672227, 11968160...   \n",
       "\n",
       "                                         sent_scores compound  comp_score  \\\n",
       "0  {'neg': 0.124, 'neu': 0.604, 'pos': 0.272, 'co...   0.7089         pos   \n",
       "1  {'neg': 0.025, 'neu': 0.472, 'pos': 0.503, 'co...   0.9877         pos   \n",
       "2  {'neg': 0.0, 'neu': 0.317, 'pos': 0.683, 'comp...   0.9442         pos   \n",
       "3  {'neg': 0.0, 'neu': 0.597, 'pos': 0.403, 'comp...   0.8957         pos   \n",
       "4  {'neg': 0.0, 'neu': 0.462, 'pos': 0.538, 'comp...   0.8519         pos   \n",
       "5  {'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound...   0.6124         pos   \n",
       "6  {'neg': 0.227, 'neu': 0.56, 'pos': 0.213, 'com...  -0.1027         neg   \n",
       "7  {'neg': 0.0, 'neu': 0.757, 'pos': 0.243, 'comp...   0.8979         pos   \n",
       "8  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   0.0000         pos   \n",
       "9  {'neg': 0.0, 'neu': 0.784, 'pos': 0.216, 'comp...   0.5106         pos   \n",
       "\n",
       "                                     sum_sent_scores sum_compound  \\\n",
       "0  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...       0.5994   \n",
       "1  {'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'comp...       0.4215   \n",
       "2  {'neg': 0.0, 'neu': 0.196, 'pos': 0.804, 'comp...       0.6249   \n",
       "3  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...       0.5994   \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...       0.0000   \n",
       "5  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...       0.0000   \n",
       "6  {'neg': 0.0, 'neu': 0.543, 'pos': 0.457, 'comp...       0.6369   \n",
       "7  {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'comp...       0.6369   \n",
       "8  {'neg': 0.0, 'neu': 0.227, 'pos': 0.773, 'comp...       0.5267   \n",
       "9  {'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'comp...       0.4215   \n",
       "\n",
       "   sum_comp_score  \n",
       "0             pos  \n",
       "1             pos  \n",
       "2             pos  \n",
       "3             pos  \n",
       "4             pos  \n",
       "5             pos  \n",
       "6             pos  \n",
       "7             pos  \n",
       "8             pos  \n",
       "9             pos  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new['comp_score'] = train_new['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new['sum_sent_scores']= train_new['summary'].apply(lambda review: sid.polarity_scores(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new['sum_compound']  = train_new['sum_sent_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "train_new['sum_comp_score'] = train_new['sum_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment_scores(data):\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    data['review_sent_scores'] = data['reviewText'].apply(lambda review: sid.polarity_scores(str(review)))\n",
    "    data['sum_sent_scores']= data['summary'].apply(lambda summary: sid.polarity_scores(str(summary)))\n",
    "     \n",
    "    data['review_compound']  = train_new['review_sent_scores'].apply(lambda score_dict: score_dict['compound'])    \n",
    "    data['sum_compound']  = data['sum_sent_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "    \n",
    "    data['review_comp_score'] = train_new['review_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "    data['sum_comp_score'] = data['sum_compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "awe_vec = awesome_products(train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon-id\n",
      "-9217723718720870868    0\n",
      "-9215746463819797371    1\n",
      "-9213978596308513604    0\n",
      "-9211290576571923870    0\n",
      "-9208769561690910545    0\n",
      "-9208198150317071838    1\n",
      "-9207623668807116759    1\n",
      "-9203846742259231188    0\n",
      "-9202734100323412002    1\n",
      "-9198322104429679287    0\n",
      "-9196816809534721621    1\n",
      "-9195787036214670721    0\n",
      "-9195715895034041804    0\n",
      "-9185450485401072051    0\n",
      "-9185028087385392712    1\n",
      "-9184754614868031288    0\n",
      "-9181938828093623181    0\n",
      "-9177871146610584170    0\n",
      "-9175284111494784505    1\n",
      "-9171391896663756617    1\n",
      "-9171133252298665868    0\n",
      "-9170624073749960483    0\n",
      "-9168022306965593241    0\n",
      "-9164485698573154140    1\n",
      "-9163887239523687605    1\n",
      "-9163211980141980025    1\n",
      "-9163010614846223784    0\n",
      "-9162759304190308282    0\n",
      "-9161926972130019940    0\n",
      "-9157117359304219671    1\n",
      "                       ..\n",
      " 9182231784957840868    0\n",
      " 9183110027749334191    0\n",
      " 9183152728540863858    0\n",
      " 9183587774518525868    1\n",
      " 9184679746257273506    1\n",
      " 9187454525560420789    1\n",
      " 9191699717616638912    1\n",
      " 9192303539297499270    1\n",
      " 9193396673702558183    1\n",
      " 9194212252781986285    1\n",
      " 9195028134778603244    1\n",
      " 9199259431832163867    1\n",
      " 9200528860526857499    1\n",
      " 9202286158489947905    0\n",
      " 9205628540934353815    1\n",
      " 9206943141300942065    1\n",
      " 9207473202971945942    1\n",
      " 9208467423485772129    0\n",
      " 9211171646322169604    1\n",
      " 9211338909954962026    1\n",
      " 9211922862265284272    0\n",
      " 9212043675364570807    1\n",
      " 9213747440163065679    1\n",
      " 9216098654890788813    0\n",
      " 9217670041811029322    0\n",
      " 9218870320655141661    0\n",
      " 9221578337502519209    1\n",
      " 9221615570697142155    1\n",
      " 9221801008952598876    0\n",
      " 9222652928856141170    0\n",
      "Name: overall, Length: 10543, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(awe_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6181\n",
       "0    4362\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awe_vec.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def correct_oversampling(x_train, y_train):\n",
    "    print(\"Fixing imbalance of class samples\")\n",
    "    sm = SMOTE(random_state=27)\n",
    "    x_train_new, y_train_new = sm.fit_resample(x_train, y_train)\n",
    "    print(\"Done\")\n",
    "    return x_train_new, y_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
